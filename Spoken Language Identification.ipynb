{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soundfile in /Users/fahmisalman/.conda/envs/Anaconda3.6/lib/python3.6/site-packages (0.10.2)\n",
      "Requirement already satisfied: cffi>=1.0 in /Users/fahmisalman/.conda/envs/Anaconda3.6/lib/python3.6/site-packages (from soundfile) (1.11.5)\n",
      "Requirement already satisfied: pycparser in /Users/fahmisalman/.conda/envs/Anaconda3.6/lib/python3.6/site-packages (from cffi>=1.0->soundfile) (2.18)\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: sklearn in /Users/fahmisalman/.conda/envs/Anaconda3.6/lib/python3.6/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/fahmisalman/.conda/envs/Anaconda3.6/lib/python3.6/site-packages (from sklearn) (0.20.0)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /Users/fahmisalman/.conda/envs/Anaconda3.6/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.15.1)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /Users/fahmisalman/.conda/envs/Anaconda3.6/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tensorflow in /Users/fahmisalman/.conda/envs/Anaconda3.6/lib/python3.6/site-packages (1.9.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /Users/fahmisalman/.conda/envs/Anaconda3.6/lib/python3.6/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in /Users/fahmisalman/.conda/envs/Anaconda3.6/lib/python3.6/site-packages (from tensorflow) (0.7.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/fahmisalman/.conda/envs/Anaconda3.6/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: setuptools<=39.1.0 in /Users/fahmisalman/.conda/envs/Anaconda3.6/lib/python3.6/site-packages (from tensorflow) (39.1.0)\n",
      "Requirement already satisfied: tensorboard<1.10.0,>=1.9.0 in /Users/fahmisalman/.conda/envs/Anaconda3.6/lib/python3.6/site-packages (from tensorflow) (1.9.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/fahmisalman/.conda/envs/Anaconda3.6/lib/python3.6/site-packages (from tensorflow) (1.15.1)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /Users/fahmisalman/.conda/envs/Anaconda3.6/lib/python3.6/site-packages (from tensorflow) (0.7.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/fahmisalman/.conda/envs/Anaconda3.6/lib/python3.6/site-packages (from tensorflow) (0.31.1)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /Users/fahmisalman/.conda/envs/Anaconda3.6/lib/python3.6/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/fahmisalman/.conda/envs/Anaconda3.6/lib/python3.6/site-packages (from tensorflow) (1.11.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /Users/fahmisalman/.conda/envs/Anaconda3.6/lib/python3.6/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /Users/fahmisalman/.conda/envs/Anaconda3.6/lib/python3.6/site-packages (from tensorboard<1.10.0,>=1.9.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/fahmisalman/.conda/envs/Anaconda3.6/lib/python3.6/site-packages (from tensorboard<1.10.0,>=1.9.0->tensorflow) (3.0.1)\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: scipy in /Users/fahmisalman/.conda/envs/Anaconda3.6/lib/python3.6/site-packages (1.1.0)\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: numpy in /Users/fahmisalman/.conda/envs/Anaconda3.6/lib/python3.6/site-packages (1.15.1)\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pandas in /Users/fahmisalman/.conda/envs/Anaconda3.6/lib/python3.6/site-packages (0.23.4)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /Users/fahmisalman/.conda/envs/Anaconda3.6/lib/python3.6/site-packages (from pandas) (2.7.3)\n",
      "Requirement already satisfied: pytz>=2011k in /Users/fahmisalman/.conda/envs/Anaconda3.6/lib/python3.6/site-packages (from pandas) (2018.5)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /Users/fahmisalman/.conda/envs/Anaconda3.6/lib/python3.6/site-packages (from pandas) (1.15.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/fahmisalman/.conda/envs/Anaconda3.6/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas) (1.11.0)\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install soundfile\n",
    "!pip install sklearn\n",
    "!pip install tensorflow\n",
    "!pip install scipy\n",
    "!pip install numpy\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import scipy.signal as signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series</th>\n",
       "      <th>languange</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.00274658203125, 0.004486083984375, 0.010803...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.07891845703125, 0.09246826171875, 0.1100769...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.007354736328125, -0.007415771484375, -0.00...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.038848876953125, 0.04473876953125, 0.055511...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.000335693359375, -0.000244140625, 0.0032043...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              series languange\n",
       "0  [0.00274658203125, 0.004486083984375, 0.010803...        de\n",
       "1  [0.07891845703125, 0.09246826171875, 0.1100769...        de\n",
       "2  [-0.007354736328125, -0.007415771484375, -0.00...        de\n",
       "3  [0.038848876953125, 0.04473876953125, 0.055511...        de\n",
       "4  [0.000335693359375, -0.000244140625, 0.0032043...        de"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = 'dataset/train/'\n",
    "\n",
    "series = []\n",
    "length = []\n",
    "for filename in os.listdir(train_path):\n",
    "    flac, samplerate = sf.read(train_path+filename)\n",
    "    series.append(flac)\n",
    "    length.append(samplerate)\n",
    "    \n",
    "label = []\n",
    "for filename in os.listdir(train_path):\n",
    "    label.append(filename[:2])\n",
    "    \n",
    "data = {'series': series,\n",
    "       'languange':label}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series</th>\n",
       "      <th>languange</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0025634765625, 0.002227783203125, 0.0018005...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.00830078125, 0.01513671875, -0.02392578125...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[6.103515625e-05, 0.0, 0.0, 6.103515625e-05, 0...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0067138671875, 0.000152587890625, 0.0022888...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.03125, 0.0198974609375, 0.015869140625, 0.0...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              series languange\n",
       "0  [0.0025634765625, 0.002227783203125, 0.0018005...        de\n",
       "1  [-0.00830078125, 0.01513671875, -0.02392578125...        de\n",
       "2  [6.103515625e-05, 0.0, 0.0, 6.103515625e-05, 0...        de\n",
       "3  [0.0067138671875, 0.000152587890625, 0.0022888...        de\n",
       "4  [0.03125, 0.0198974609375, 0.015869140625, 0.0...        de"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_path = 'dataset/test/'\n",
    "\n",
    "series = []\n",
    "length = []\n",
    "for filename in os.listdir(test_path):\n",
    "    flac, samplerate = sf.read(test_path+filename)\n",
    "    series.append(flac)\n",
    "    length.append(samplerate)\n",
    "    \n",
    "label = []\n",
    "for filename in os.listdir(test_path):\n",
    "    label.append(filename[:2])\n",
    "    \n",
    "data_test = {'series': series,\n",
    "       'languange':label}\n",
    "\n",
    "df_test = pd.DataFrame(data_test)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import librosa\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    return label_binarizer.transform(x)\n",
    "\n",
    "x = np.array(df['series'])\n",
    "x_t = np.array(df_test['series'])\n",
    "y_train = df['languange']\n",
    "y_test = df_test['languange']\n",
    "\n",
    "# Convert Pandas DataFrame into Numpy array\n",
    "x_train = np.zeros((len(x), 431))\n",
    "for i in range(len(x_train)):\n",
    "    a = librosa.feature.zero_crossing_rate(x[i])\n",
    "    x_train[i] = a\n",
    "    \n",
    "x_test = np.zeros((len(x_t), 431))\n",
    "for i in range(len(x_test)):\n",
    "    a = librosa.feature.zero_crossing_rate(x_t[i])\n",
    "    x_test[i] = a\n",
    "    \n",
    "# Convert label into one hot encoding\n",
    "label_binarizer = LabelBinarizer()\n",
    "label_binarizer.fit(y_train)\n",
    "y_train_onehot = one_hot_encode(y_train)\n",
    "\n",
    "label_binarizer.fit(y_test)\n",
    "y_test_onehot = one_hot_encode(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.8148148148148148\n",
      "Test : 0.7222222222222222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(x_train, y_train)\n",
    "print('Train:', clf.score(x_train, y_train))\n",
    "print('Test :', clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1.0\n",
      "Test : 0.6481481481481481\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(10), random_state=1)\n",
    "\n",
    "clf.fit(x_train, y_train_onehot)\n",
    "print('Train:', clf.score(x_train, y_train_onehot))\n",
    "print('Test :', clf.score(x_test, y_test_onehot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.4722222222222222\n",
      "Test : 0.37962962962962965\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=5)\n",
    "clf.fit(x_train, y_train)\n",
    "print('Train:', clf.score(x_train, y_train))\n",
    "print('Test :', clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fahmisalman/.conda/envs/Anaconda3.6/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.6782407407407407\n",
      "Test : 0.6111111111111112\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC()\n",
    "clf.fit(x_train, y_train)\n",
    "print('Train:', clf.score(x_train, y_train))\n",
    "print('Test :', clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_6 (GRU)                  (None, 431, 256)          198144    \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  (None, 128)               147840    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 346,371\n",
      "Trainable params: 346,371\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "432/432 [==============================] - 19s 44ms/step - loss: 0.2191 - acc: 0.4282\n",
      "Epoch 2/100\n",
      "432/432 [==============================] - 15s 36ms/step - loss: 0.2003 - acc: 0.4630\n",
      "Epoch 3/100\n",
      "432/432 [==============================] - 16s 36ms/step - loss: 0.2470 - acc: 0.5185\n",
      "Epoch 4/100\n",
      "432/432 [==============================] - 16s 38ms/step - loss: 0.3123 - acc: 0.4769\n",
      "Epoch 5/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.3209 - acc: 0.4213\n",
      "Epoch 6/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.3240 - acc: 0.4630\n",
      "Epoch 7/100\n",
      "432/432 [==============================] - 16s 38ms/step - loss: 0.3263 - acc: 0.4606\n",
      "Epoch 8/100\n",
      "432/432 [==============================] - 17s 39ms/step - loss: 0.3289 - acc: 0.4583\n",
      "Epoch 9/100\n",
      "432/432 [==============================] - 17s 39ms/step - loss: 0.3282 - acc: 0.4583\n",
      "Epoch 10/100\n",
      "432/432 [==============================] - 17s 40ms/step - loss: 0.3284 - acc: 0.4583\n",
      "Epoch 11/100\n",
      "432/432 [==============================] - 16s 36ms/step - loss: 0.3291 - acc: 0.4583\n",
      "Epoch 12/100\n",
      "432/432 [==============================] - 17s 39ms/step - loss: 0.3284 - acc: 0.4583\n",
      "Epoch 13/100\n",
      "432/432 [==============================] - 17s 39ms/step - loss: 0.3287 - acc: 0.4583\n",
      "Epoch 14/100\n",
      "432/432 [==============================] - 16s 38ms/step - loss: 0.3090 - acc: 0.4676\n",
      "Epoch 15/100\n",
      "432/432 [==============================] - 16s 38ms/step - loss: 0.2212 - acc: 0.5255\n",
      "Epoch 16/100\n",
      "432/432 [==============================] - 16s 38ms/step - loss: 0.2290 - acc: 0.4120\n",
      "Epoch 17/100\n",
      "432/432 [==============================] - 16s 38ms/step - loss: 0.2797 - acc: 0.3148\n",
      "Epoch 18/100\n",
      "432/432 [==============================] - 17s 39ms/step - loss: 0.2789 - acc: 0.3287\n",
      "Epoch 19/100\n",
      "432/432 [==============================] - 17s 38ms/step - loss: 0.2784 - acc: 0.3333\n",
      "Epoch 20/100\n",
      "432/432 [==============================] - 16s 38ms/step - loss: 0.2784 - acc: 0.3125\n",
      "Epoch 21/100\n",
      "432/432 [==============================] - 17s 39ms/step - loss: 0.2786 - acc: 0.3218\n",
      "Epoch 22/100\n",
      "432/432 [==============================] - 16s 38ms/step - loss: 0.2794 - acc: 0.3333\n",
      "Epoch 23/100\n",
      "432/432 [==============================] - 16s 38ms/step - loss: 0.2788 - acc: 0.3333\n",
      "Epoch 24/100\n",
      "432/432 [==============================] - 17s 38ms/step - loss: 0.2781 - acc: 0.3426\n",
      "Epoch 25/100\n",
      "432/432 [==============================] - 16s 38ms/step - loss: 0.2783 - acc: 0.3171\n",
      "Epoch 26/100\n",
      "432/432 [==============================] - 17s 38ms/step - loss: 0.2777 - acc: 0.3588\n",
      "Epoch 27/100\n",
      "432/432 [==============================] - 17s 38ms/step - loss: 0.2777 - acc: 0.3681\n",
      "Epoch 28/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2778 - acc: 0.3426\n",
      "Epoch 29/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2781 - acc: 0.3148\n",
      "Epoch 30/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2779 - acc: 0.3380\n",
      "Epoch 31/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2784 - acc: 0.3218\n",
      "Epoch 32/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2779 - acc: 0.3310\n",
      "Epoch 33/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2780 - acc: 0.3333\n",
      "Epoch 34/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2783 - acc: 0.3310\n",
      "Epoch 35/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2789 - acc: 0.3310\n",
      "Epoch 36/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2785 - acc: 0.3171\n",
      "Epoch 37/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2778 - acc: 0.3310\n",
      "Epoch 38/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2778 - acc: 0.3356\n",
      "Epoch 39/100\n",
      "432/432 [==============================] - 16s 36ms/step - loss: 0.2786 - acc: 0.3102\n",
      "Epoch 40/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2781 - acc: 0.3472\n",
      "Epoch 41/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2786 - acc: 0.2940\n",
      "Epoch 42/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2776 - acc: 0.3681\n",
      "Epoch 43/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2777 - acc: 0.3403\n",
      "Epoch 44/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2778 - acc: 0.3426\n",
      "Epoch 45/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2782 - acc: 0.3102\n",
      "Epoch 46/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2775 - acc: 0.3472\n",
      "Epoch 47/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2790 - acc: 0.3380\n",
      "Epoch 48/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2806 - acc: 0.3287\n",
      "Epoch 49/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2785 - acc: 0.3287\n",
      "Epoch 50/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2788 - acc: 0.3403\n",
      "Epoch 51/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2784 - acc: 0.3611\n",
      "Epoch 52/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2778 - acc: 0.3472\n",
      "Epoch 53/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2774 - acc: 0.3449\n",
      "Epoch 54/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2784 - acc: 0.3495\n",
      "Epoch 55/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2772 - acc: 0.3704\n",
      "Epoch 56/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2773 - acc: 0.3634\n",
      "Epoch 57/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2778 - acc: 0.3657\n",
      "Epoch 58/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2780 - acc: 0.3472\n",
      "Epoch 59/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2778 - acc: 0.3565\n",
      "Epoch 60/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2770 - acc: 0.3819\n",
      "Epoch 61/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2768 - acc: 0.3958\n",
      "Epoch 62/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2774 - acc: 0.3495\n",
      "Epoch 63/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2769 - acc: 0.3866\n",
      "Epoch 64/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2779 - acc: 0.3611\n",
      "Epoch 65/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2776 - acc: 0.3681\n",
      "Epoch 66/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2767 - acc: 0.3727\n",
      "Epoch 67/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2766 - acc: 0.3727\n",
      "Epoch 68/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2767 - acc: 0.3843\n",
      "Epoch 69/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2765 - acc: 0.3866\n",
      "Epoch 70/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2774 - acc: 0.3704\n",
      "Epoch 71/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2421 - acc: 0.4630\n",
      "Epoch 72/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.1931 - acc: 0.5509\n",
      "Epoch 73/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.1914 - acc: 0.5532\n",
      "Epoch 74/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.1938 - acc: 0.5509\n",
      "Epoch 75/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2273 - acc: 0.5278\n",
      "Epoch 76/100\n",
      "432/432 [==============================] - 16s 36ms/step - loss: 0.4002 - acc: 0.3750\n",
      "Epoch 77/100\n",
      "432/432 [==============================] - 16s 36ms/step - loss: 0.4322 - acc: 0.3426\n",
      "Epoch 78/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.4319 - acc: 0.3495\n",
      "Epoch 79/100\n",
      "432/432 [==============================] - 16s 36ms/step - loss: 0.4256 - acc: 0.3588\n",
      "Epoch 80/100\n",
      "432/432 [==============================] - 16s 36ms/step - loss: 0.3607 - acc: 0.3449\n",
      "Epoch 81/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2786 - acc: 0.3657\n",
      "Epoch 82/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2684 - acc: 0.3426\n",
      "Epoch 83/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2495 - acc: 0.3310\n",
      "Epoch 84/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2246 - acc: 0.3472\n",
      "Epoch 85/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2222 - acc: 0.3773\n",
      "Epoch 86/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2211 - acc: 0.3704\n",
      "Epoch 87/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2199 - acc: 0.3611\n",
      "Epoch 88/100\n",
      "432/432 [==============================] - 16s 36ms/step - loss: 0.2187 - acc: 0.3611\n",
      "Epoch 89/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2193 - acc: 0.3750\n",
      "Epoch 90/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2187 - acc: 0.3843\n",
      "Epoch 91/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2166 - acc: 0.3495\n",
      "Epoch 92/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2150 - acc: 0.4745\n",
      "Epoch 93/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2133 - acc: 0.4375\n",
      "Epoch 94/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2136 - acc: 0.3681\n",
      "Epoch 95/100\n",
      "432/432 [==============================] - 16s 36ms/step - loss: 0.2157 - acc: 0.3796\n",
      "Epoch 96/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2159 - acc: 0.4005\n",
      "Epoch 97/100\n",
      "432/432 [==============================] - 16s 37ms/step - loss: 0.2116 - acc: 0.4282\n",
      "Epoch 98/100\n",
      "432/432 [==============================] - 16s 36ms/step - loss: 0.2097 - acc: 0.4745\n",
      "Epoch 99/100\n",
      "432/432 [==============================] - 16s 36ms/step - loss: 0.2083 - acc: 0.4167\n",
      "Epoch 100/100\n",
      "432/432 [==============================] - 16s 36ms/step - loss: 0.2092 - acc: 0.4838\n",
      "[0.20921819132787209, 0.47222222001464281]\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential, models\n",
    "from keras.layers import LSTM, Dense, Flatten, GRU\n",
    "from keras.metrics import binary_accuracy\n",
    "\n",
    "def train(x, y, epoch=20, hidden=256):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units=hidden, input_shape=(x.shape[1], 1), return_sequences=True))\n",
    "    model.add(GRU(units=128))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    model.fit(x, y, epochs=epoch, batch_size=64, verbose=1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate(x, y, model):\n",
    "    mse = model.evaluate(x, y, verbose=0)\n",
    "    return mse\n",
    "\n",
    "\n",
    "def save_model(model, s):\n",
    "    model_json = model.to_json()\n",
    "    with open(\"model/%s.json\" % s, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(\"model/%s.h5\" % s)\n",
    "\n",
    "\n",
    "def load_model(s):\n",
    "    model_json = open('model/%s.json' % s, 'r').read()\n",
    "    model = models.model_from_json(model_json)\n",
    "    model.load_weights(\"model/%s.h5\" % s)\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(x, model):\n",
    "    return model.predict(x)\n",
    "\n",
    "\n",
    "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "model = train(x_train, y_train_onehot, epoch=100)\n",
    "print(evaluate(x_test, y_test_onehot, model))\n",
    "save_model(model, 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
